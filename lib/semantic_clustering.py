#!/usr/bin/env python3
"""
è¯­ä¹‰èšç±»æœåŠ¡ - åŸºäºæ™ºè°±AI Embedding + DBSCAN
æ›¿ä»£åŸæœ‰çš„ Jaccard èšç±»ç®—æ³•ï¼Œæä¾›æ›´å¥½çš„è¯­ä¹‰ç›¸ä¼¼åº¦èšç±»
"""

import json
import sys
import os
import re
import time
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path

import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_distances

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
env_path = Path(__file__).parent.parent
load_dotenv(env_path / '.env.local')
load_dotenv(env_path / '.env')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ==================== æ•°æ®æ¸…æ´—æ¨¡å— ====================

class DataCleaner:
    """é«˜ä¿¡å™ªæ¯”æ•°æ®æ¸…æ´—å™¨"""

    # æ— æ•ˆç¤¾äº¤ç”¨è¯­ï¼ˆä¼šè¢«ç›´æ¥è¿‡æ»¤ï¼‰
    NOISE_PATTERNS = [
        r'^å“ˆ+$',           # çº¯å“ˆå“ˆå“ˆ
        r'^å˜»+$',           # çº¯å˜»å˜»å˜»
        r'^å‘µ+$',           # çº¯å‘µå‘µå‘µ
        r'^[å¥½æ£’èµ]+$',     # çº¯å¥½æ£’èµ
        r'^æ”¯æŒ+$',
        r'^åŠ æ²¹+$',
        r'^è¹²+$',
        r'^@\S+',           # @æŸäºº
        r'^è½¬å‘å¾®åš',
        r'^å·²é˜…$',
        r'^mark$',
        r'^[Mm]ark$',
        r'^æ”¶è—$',
        r'^[ğŸ‘â¤ï¸ğŸ’•ğŸ‰ğŸ˜€ğŸ˜ğŸ˜‚ğŸ¤£ğŸ˜ƒğŸ˜„ğŸ˜…ğŸ˜†ğŸ˜ŠğŸ˜‹ğŸ˜]+$',  # çº¯è¡¨æƒ…
    ]

    # æ— æ•ˆçŸ­è¯­åˆ—è¡¨
    NOISE_PHRASES = [
        'å¥½å¬', 'å¥½çœ‹', 'çœŸå¥½', 'ä¸é”™', 'å¯ä»¥', 'å‰å®³', 'ç‰›', 'ç»äº†',
        'å“ˆå“ˆå“ˆ', 'å“ˆå“ˆå“ˆå“ˆ', 'ç¬‘æ­»', 'ç¬‘äº†', 'å¤ªå¥½ç¬‘', 'ç»ç»å­',
        'è¹²', 'è¹²ä¸€ä¸ª', 'ç­‰æ›´æ–°', 'å‚¬æ›´', 'ä»€ä¹ˆæ—¶å€™æ›´æ–°',
        'ç¬¬ä¸€', 'æ²™å‘', 'å‰æ’', 'æ¥äº†', 'æ‰“å¡', 'ç­¾åˆ°',
        'æ”¯æŒ', 'åŠ æ²¹', 'å†²', 'å†²å†²å†²', 'å¥¥åˆ©ç»™',
        'yyds', 'æ°¸è¿œçš„ç¥', 'å¤ªå¼ºäº†', 'å¤ªæ£’äº†',
    ]

    # ç™½åå•å…³é”®è¯ï¼ˆä¼˜å…ˆä¿ç•™ï¼‰- ç—›ç‚¹ç›¸å…³è¯æ±‡
    WHITELIST_KEYWORDS = [
        'æ€ä¹ˆ', 'éš¾', 'æ±‚', 'å‘', 'è´µ', 'éº»çƒ¦', 'å¯¼è‡´', 'å¸Œæœ›',
        'ä»£æ›¿', 'ä¸æ‡‚', 'åæ‚”', 'é¿é›·', 'è¸©å‘', 'é—®é¢˜', 'è§£å†³',
        'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'å“ªé‡Œ', 'æ¨è', 'å»ºè®®', 'æ•™ç¨‹',
        'ä¸ä¼š', 'å­¦ä¸ä¼š', 'å¤ªéš¾', 'æä¸æ‡‚', 'çœ‹ä¸æ‡‚',
        'ä¾¿å®œ', 'å¹³æ›¿', 'æ›¿ä»£', 'çœé’±', 'åˆ’ç®—',
        'åæ§½', 'å·®è¯„', 'é€€æ¬¾', 'å”®å', 'å®¢æœ', 'è´¨é‡',
        'bug', 'BUG', 'å¡', 'é—ªé€€', 'å´©æºƒ', 'æŠ¥é”™',
    ]

    def __init__(self, min_length: int = 4):
        self.min_length = min_length
        self.noise_regexes = [re.compile(p) for p in self.NOISE_PATTERNS]

    def is_noise(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºå™ªéŸ³"""
        text = text.strip()

        # é•¿åº¦è¿‡çŸ­
        if len(text) < self.min_length:
            return True

        # åŒ¹é…å™ªéŸ³æ­£åˆ™
        for regex in self.noise_regexes:
            if regex.match(text):
                return True

        # åŒ¹é…å™ªéŸ³çŸ­è¯­
        text_lower = text.lower()
        for phrase in self.NOISE_PHRASES:
            if text_lower == phrase.lower():
                return True

        return False

    def has_whitelist_keyword(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«ç™½åå•å…³é”®è¯"""
        for keyword in self.WHITELIST_KEYWORDS:
            if keyword in text:
                return True
        return False

    def calculate_score(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬è´¨é‡åˆ†æ•°ï¼ˆç”¨äºæ’åºï¼‰"""
        score = 1.0

        # ç™½åå•åŠ æƒ
        if self.has_whitelist_keyword(text):
            score += 2.0

        # é•¿åº¦åŠ æƒï¼ˆ10-100å­—ç¬¦æœ€ä½³ï¼‰
        length = len(text)
        if 10 <= length <= 100:
            score += 0.5
        elif length > 200:
            score -= 0.3

        # åŒ…å«é—®å·åŠ æƒï¼ˆå¯èƒ½æ˜¯é—®é¢˜/ç—›ç‚¹ï¼‰
        if '?' in text or 'ï¼Ÿ' in text:
            score += 0.5

        return score

    def clean(self, texts: List[str]) -> Tuple[List[str], List[float]]:
        """
        æ¸…æ´—æ–‡æœ¬åˆ—è¡¨
        è¿”å›: (æ¸…æ´—åçš„æ–‡æœ¬åˆ—è¡¨, å¯¹åº”çš„è´¨é‡åˆ†æ•°)
        """
        cleaned = []
        scores = []
        seen = set()  # å»é‡

        for text in texts:
            text = text.strip()

            # è·³è¿‡ç©ºæ–‡æœ¬
            if not text:
                continue

            # å»é‡
            if text in seen:
                continue
            seen.add(text)

            # è·³è¿‡å™ªéŸ³
            if self.is_noise(text):
                continue

            score = self.calculate_score(text)
            cleaned.append(text)
            scores.append(score)

        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {len(texts)} -> {len(cleaned)} æ¡ (è¿‡æ»¤äº† {len(texts) - len(cleaned)} æ¡å™ªéŸ³)")
        return cleaned, scores


# ==================== Embedding æ¨¡å— ====================

class ZhipuEmbedding:
    """æ™ºè°±AI Embedding æœåŠ¡"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv('GLM_API_KEY')
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° GLM_API_KEYï¼Œè¯·åœ¨ .env.local æˆ–ç¯å¢ƒå˜é‡ä¸­é…ç½®")

        self.base_url = "https://open.bigmodel.cn/api/paas/v4/embeddings"
        self.model = os.getenv('GLM_EMBEDDING_MODEL', 'embedding-3')
        self.batch_size = 25  # æ¯æ‰¹æœ€å¤§æ•°é‡
        self.rate_limit_delay = 0.5  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

    def _get_embedding_batch(self, texts: List[str]) -> List[List[float]]:
        """è·å–ä¸€æ‰¹æ–‡æœ¬çš„ embedding"""
        import urllib.request
        import urllib.error

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }

        # æ™ºè°± API éœ€è¦é€ä¸ªè¯·æ±‚
        embeddings = []
        for text in texts:
            data = json.dumps({
                'model': self.model,
                'input': text
            }).encode('utf-8')

            req = urllib.request.Request(self.base_url, data=data, headers=headers)

            try:
                with urllib.request.urlopen(req, timeout=30) as response:
                    result = json.loads(response.read().decode('utf-8'))
                    embedding = result['data'][0]['embedding']
                    embeddings.append(embedding)
            except urllib.error.HTTPError as e:
                error_body = e.read().decode('utf-8')
                logger.error(f"API é”™è¯¯: {e.code} - {error_body}")
                raise
            except Exception as e:
                logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
                raise

            # æ·»åŠ å»¶è¿Ÿé¿å…é™æµ
            time.sleep(self.rate_limit_delay)

        return embeddings

    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        è·å–æ–‡æœ¬åˆ—è¡¨çš„ embedding å‘é‡
        è‡ªåŠ¨å¤„ç†æ‰¹é‡è¯·æ±‚å’Œé™æµ
        """
        if not texts:
            return np.array([])

        all_embeddings = []
        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size

        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            logger.info(f"æ­£åœ¨è·å– Embedding: æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ¡)")

            embeddings = self._get_embedding_batch(batch)
            all_embeddings.extend(embeddings)

        return np.array(all_embeddings)


# ==================== èšç±»æ¨¡å— ====================

class SemanticClusterer:
    """åŸºäº DBSCAN çš„è¯­ä¹‰èšç±»å™¨"""

    def __init__(self, eps: float = 0.4, min_samples: int = 2):
        """
        å‚æ•°:
            eps: DBSCAN çš„é‚»åŸŸåŠå¾„ï¼ˆåŸºäºä½™å¼¦è·ç¦»ï¼Œ0.3-0.5 è¾ƒå¥½ï¼‰
            min_samples: å½¢æˆèšç±»çš„æœ€å°æ ·æœ¬æ•°
        """
        self.eps = eps
        self.min_samples = min_samples

    def cluster(self, embeddings: np.ndarray, texts: List[str], scores: Optional[List[float]] = None) -> List[Dict]:
        """
        æ‰§è¡Œ DBSCAN èšç±»

        è¿”å›æ ¼å¼:
        [
            {
                "representative_text": "è·ç¦»èšç±»ä¸­å¿ƒæœ€è¿‘çš„æ–‡æœ¬",
                "size": èšç±»å¤§å°,
                "texts": ["æ–‡æœ¬1", "æ–‡æœ¬2", ...]
            },
            ...
        ]
        """
        if len(embeddings) == 0:
            return []

        if scores is None:
            scores = [1.0] * len(texts)

        # è®¡ç®—ä½™å¼¦è·ç¦»çŸ©é˜µ
        logger.info("æ­£åœ¨è®¡ç®—ä½™å¼¦è·ç¦»çŸ©é˜µ...")
        distance_matrix = cosine_distances(embeddings)

        # DBSCAN èšç±»
        logger.info(f"æ­£åœ¨æ‰§è¡Œ DBSCAN èšç±» (eps={self.eps}, min_samples={self.min_samples})...")
        dbscan = DBSCAN(eps=self.eps, min_samples=self.min_samples, metric='precomputed')
        labels = dbscan.fit_predict(distance_matrix)

        # ç»Ÿè®¡èšç±»ç»“æœ
        unique_labels = set(labels)
        n_clusters = len([l for l in unique_labels if l != -1])
        n_noise = list(labels).count(-1)
        logger.info(f"èšç±»å®Œæˆ: {n_clusters} ä¸ªèšç±», {n_noise} ä¸ªå™ªéŸ³ç‚¹")

        # æ„å»ºèšç±»ç»“æœ
        clusters = {}
        for idx, label in enumerate(labels):
            if label == -1:  # å™ªéŸ³ç‚¹å•ç‹¬å¤„ç†
                continue
            if label not in clusters:
                clusters[label] = {
                    'indices': [],
                    'texts': [],
                    'scores': []
                }
            clusters[label]['indices'].append(idx)
            clusters[label]['texts'].append(texts[idx])
            clusters[label]['scores'].append(scores[idx])

        # ä¸ºæ¯ä¸ªèšç±»æ‰¾å‡ºä»£è¡¨æ€§æ–‡æœ¬
        results = []
        for label, cluster_data in clusters.items():
            indices = cluster_data['indices']
            cluster_texts = cluster_data['texts']
            cluster_scores = cluster_data['scores']

            # è®¡ç®—èšç±»ä¸­å¿ƒ
            cluster_embeddings = embeddings[indices]
            centroid = np.mean(cluster_embeddings, axis=0)

            # æ‰¾åˆ°è·ç¦»ä¸­å¿ƒæœ€è¿‘çš„æ–‡æœ¬
            distances_to_center = cosine_distances([centroid], cluster_embeddings)[0]

            # ç»¼åˆè·ç¦»å’Œè´¨é‡åˆ†æ•°é€‰æ‹©ä»£è¡¨æ–‡æœ¬
            combined_scores = []
            for i, (dist, score) in enumerate(zip(distances_to_center, cluster_scores)):
                # è·ç¦»è¶Šå°è¶Šå¥½ï¼ˆå–åï¼‰ï¼Œåˆ†æ•°è¶Šé«˜è¶Šå¥½
                combined = -dist + score * 0.3
                combined_scores.append(combined)

            best_idx = np.argmax(combined_scores)
            representative_text = cluster_texts[best_idx]

            # å»é‡åçš„æ–‡æœ¬åˆ—è¡¨
            unique_texts = list(dict.fromkeys(cluster_texts))

            results.append({
                'representative_text': representative_text,
                'size': len(unique_texts),
                'texts': unique_texts
            })

        # æŒ‰èšç±»å¤§å°æ’åº
        results.sort(key=lambda x: x['size'], reverse=True)

        # å¤„ç†å™ªéŸ³ç‚¹ï¼šå°†é«˜è´¨é‡çš„å™ªéŸ³ç‚¹ä½œä¸ºå•ç‹¬èšç±»
        noise_indices = [i for i, label in enumerate(labels) if label == -1]
        for idx in noise_indices:
            if scores[idx] >= 2.0:  # åªä¿ç•™é«˜è´¨é‡çš„å™ªéŸ³ç‚¹
                results.append({
                    'representative_text': texts[idx],
                    'size': 1,
                    'texts': [texts[idx]]
                })

        return results


# ==================== ä¸»æµç¨‹ ====================

def process_texts(
    texts: List[str],
    eps: float = 0.4,
    min_samples: int = 2,
    min_length: int = 4
) -> List[Dict]:
    """
    å®Œæ•´çš„æ–‡æœ¬å¤„ç†æµç¨‹ï¼šæ¸…æ´— -> Embedding -> èšç±»
    """
    if not texts:
        return []

    # 1. æ•°æ®æ¸…æ´—
    logger.info("å¼€å§‹æ•°æ®æ¸…æ´—...")
    cleaner = DataCleaner(min_length=min_length)
    cleaned_texts, scores = cleaner.clean(texts)

    if not cleaned_texts:
        logger.warning("æ¸…æ´—åæ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬")
        return []

    # 2. è·å– Embedding
    logger.info("å¼€å§‹è·å– Embedding...")
    embedder = ZhipuEmbedding()
    embeddings = embedder.get_embeddings(cleaned_texts)

    # 3. DBSCAN èšç±»
    logger.info("å¼€å§‹è¯­ä¹‰èšç±»...")
    clusterer = SemanticClusterer(eps=eps, min_samples=min_samples)
    clusters = clusterer.cluster(embeddings, cleaned_texts, scores)

    logger.info(f"å¤„ç†å®Œæˆï¼Œå…± {len(clusters)} ä¸ªèšç±»")
    return clusters


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='è¯­ä¹‰èšç±»æœåŠ¡')
    parser.add_argument('--input', '-i', type=str, help='è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', type=str, help='è¾“å‡º JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--eps', type=float, default=0.4, help='DBSCAN eps å‚æ•°')
    parser.add_argument('--min-samples', type=int, default=2, help='DBSCAN min_samples å‚æ•°')
    parser.add_argument('--min-length', type=int, default=4, help='æœ€å°æ–‡æœ¬é•¿åº¦')
    parser.add_argument('--stdin', action='store_true', help='ä»æ ‡å‡†è¾“å…¥è¯»å– JSON')

    args = parser.parse_args()

    # è¯»å–è¾“å…¥
    if args.stdin:
        input_data = json.loads(sys.stdin.read())
    elif args.input:
        with open(args.input, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
    else:
        parser.error("è¯·æŒ‡å®š --input æˆ– --stdin")
        return

    # æ”¯æŒä¸¤ç§è¾“å…¥æ ¼å¼ï¼šç›´æ¥æ•°ç»„æˆ– {"texts": [...]}
    if isinstance(input_data, list):
        texts = input_data
    else:
        texts = input_data.get('texts', [])

    # å¤„ç†
    try:
        results = process_texts(
            texts,
            eps=args.eps,
            min_samples=args.min_samples,
            min_length=args.min_length
        )

        output = {
            'success': True,
            'clusters': results,
            'total_clusters': len(results),
            'total_texts': sum(c['size'] for c in results)
        }
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        output = {
            'success': False,
            'error': str(e),
            'clusters': []
        }

    # è¾“å‡ºç»“æœ
    result_json = json.dumps(output, ensure_ascii=False, indent=2)

    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(result_json)
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {args.output}")
    else:
        print(result_json)


if __name__ == '__main__':
    main()
